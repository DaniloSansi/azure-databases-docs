### YamlMime:FAQ
metadata:
  title: Frequently asked questions about autoscale provisioned throughput in Azure Cosmos DB 
  description: Frequently asked questions about autoscale provisioned throughput for Azure Cosmos DB databases and containers.
  author: deborahc
  ms.author: dech
  ms.service: cosmos-db
  ms.custom: ignite-2022
  ms.topic: faq
  ms.date: 04/01/2022
title: Frequently asked questions about autoscale provisioned throughput in Azure Cosmos DB
summary: |
  [!INCLUDE[NoSQL, MongoDB, Cassandra, Gremlin, Table](includes/appliesto-nosql-mongodb-cassandra-gremlin-table.md)]
  
  Azure Cosmos DB uses autoscale provisioned throughput to automatically manage and scale the request units per second (RU/s) of your database or container based on usage. This article answers commonly asked questions about autoscale.

sections:
  - name: General
    questions:        
      - question: |
          What's the difference between *autoscale* and *autopilot* in Azure Cosmos DB?
        answer: |
          *Autoscale* or *autoscale provisioned throughput* is the updated name for the feature that previously was called *autopilot*. With the current release of autoscale, we've added new features, including the ability to set custom maximum RU/s and programmatic support. 

      - question: |
          What happens to databases or containers that were created in the earlier autopilot tier model?
        answer: |
          Resources that were created with the earlier tier model are automatically supported with the new autoscale custom maximum RU/s model. The upper limit of the tier becomes the new maximum RU/s, which results in the same scale range. 
          
          For example, if you previously selected the tier that scaled between 400 and 4,000 RU/s, the database or container now shows a maximum RU/s of 4,000 RU/s, which scales between 400 and 4,000 RU/s. From here, you can change the maximum RU/s to a custom value to accommodate your workload. 

      - question: |
          What is the entry point RU/s for autoscale?
        answer: |
          Starting April 2022, you can set autoscale with maximum RU/s as low as 1,000 RU/s (scales between 100 RU/s and 1,000 RU/s). You also can set scale ranges of 200 to 2,000 RU/s and 300 to 3,000 RU/s. Previously, the entry point was 400 to 4,000 RU/s. We recommend this configuration for workloads that have low throughput requirements, but which still might scale to the maximum RU/s.      
      - question: |
          How quickly does autoscale scale up based on increases in traffic?
        answer: |
          With autoscale, the system scales the throughput (RU/s) `T` up or down within the `0.1 * Tmax` and `Tmax` range, based on incoming traffic. Because the scaling is automatic and instantaneous, at any point in time, you can consume up to the provisioned `Tmax` with no delay. 

      - question: |
          How do I determine what RU/s the system is currently scaled to?
        answer: |
          Use [Azure Monitor metrics](how-to-choose-offer.md#measure-and-monitor-your-usage) to monitor both the provisioned autoscale maximum RU/s and the current throughput (RU/s) the system is scaled to. 
          
      - question: |
          What is the pricing for autoscale?
        answer: |
          Each hour, you're billed for the highest throughput `T` the system scaled to within that hour. If your resource had no requests during the hour or didn't scale beyond `0.1 * Tmax`, you're billed for the minimum of `0.1 * Tmax`. Refer to the Azure Cosmos DB [pricing page](https://azure.microsoft.com/pricing/details/cosmos-db/) for details. 
          
      - question: |
          How does autoscale show up on my bill?
        answer: |
          In single-write region accounts, the autoscale rate per 100 RU/s is 1.5x the rate of standard (manual) provisioned throughput. On your bill, you see the existing standard provisioned throughput meter. The quantity of this meter is multiplied by 1.5. For example, if the highest RU/s the system scaled to within an hour was 6,000 RU/s, you're billed 60 &times; 1.5 = 90 units of the meter for that hour.
          
          In accounts that have multiple-write regions, the autoscale rate per 100 RU/s is the same as the rate for standard (manual) provisioned multiple-write region throughput. On your bill, you'll see the existing multiple-write regions meter. Because the rates are the same, if you use autoscale, you see the same quantity as with standard throughput.
          
      - question: |
          Does autoscale work with reserved capacity?
        answer: |
          Yes. With reserved capacity for accounts with single-write regions, the reservation discount for autoscale resources is applied to the meter usage at a ratio of 1.5 &times; the [ratio of the specific region](../cost-management-billing/reservations/understand-cosmosdb-reservation-charges.md#reservation-discount-per-region). For example, if you want to use reserved capacity to cover 10,000 autoscale RU/s, you should plan to purchase 15,000 RU/s of reserved capacity overall.
          
          Multi-write region reserved capacity works the same for autoscale and standard (manual) provisioned throughput. See [Azure Cosmos DB reserved capacity](reserved-capacity.md).
          
      - question: |
          Does autoscale work with the Azure Cosmos DB free tier?
        answer: |
          Yes. In the free tier, you can use autoscale throughput on a database or on a container. See how [free tier billing works with autoscale](understand-your-bill.md#azure-free-tier).
          
      - question: |
          Is autoscale supported for all APIs?
        answer: |
          Yes, autoscale is supported for all APIs, which includes NoSQL, Gremlin, Table, Cassandra, and MongoDB.
          
      - question: |
          Is autoscale supported for multi-region write accounts?
        answer: |
          Yes. The maximum RU/s are available in each region that's added to the Azure Cosmos DB account. 

      - question: |
          How do I enable autoscale on new databases or containers?
        answer: |
          Learn [how to enable autoscale](how-to-provision-autoscale-throughput.md).
          
      - question: |
          Can I enable autoscale on an existing database or container?
        answer: |
          Yes. You can also switch between autoscale and standard (manual) provisioned throughput. Currently, for all APIs, you can use the [Azure portal](how-to-provision-autoscale-throughput.md#enable-autoscale-on-existing-database-or-container), the [Azure CLI](scripts/cli/sql/throughput.md?toc=%2fcli%2fazure%2ftoc.json) or [PowerShell](scripts/powershell/sql/throughput.md?toc=%2fpowershell%2fmodule%2ftoc.json) to do these operations. By design, you can't use the Azure Cosmos DB client SDKs or Azure Resource Manager template to migrate between manual provisioned throughput and autoscale. However, you can use client SDKs or Azure Resource Manager templates to create new autoscale resources and to change the maximum RU/s on an existing autoscale resource.
        
      - question: |
          How does the migration between autoscale and standard (manual) provisioned throughput work?
        answer: |
          Conceptually, changing the throughput type is a two-stage process. First, you send a request to change the throughput settings to use either autoscale or manual provisioned throughput. In both cases, the system automatically determines and sets an initial RU/s value, based on the current throughput settings and storage. During this step, no user-provided RU/s value is accepted. Then, after the update is complete, you can [change the RU/s](#can-i-change-the-maximum-ru-s-on-a-database-or-container--) to suit your workload. 
          
          **Migration from standard (manual) provisioned throughput to autoscale**
          
          For a container, use the following formula to estimate the initial autoscale maximum RU/s: `MAX(1,000, current manual provisioned RU/s, maximum RU/s ever provisioned / 10, storage in GB * 100)`, rounded to the nearest 1,000 RU/s. The actual initial autoscale maximum RU/s may vary depending on your account configuration.
          
          Example #1: Suppose you have a container with 10,000 RU/s manual provisioned throughput, and 25 GB of storage. When you enable autoscale, the initial autoscale maximum RU/s is: 10,000 RU/s, which can scale between 1,000 - 10,000 RU/s. 
          
          Example #2: Suppose you have a container with 50,000 RU/s manual provisioned throughput, and 25,000 GB of storage. When you enable autoscale, the initial autoscale maximum RU/s is: 250,000 RU/s, which can scale between 25,000 - 250,000 RU/s. 
          
          **Migration from autoscale to standard (manual) provisioned throughput**
          
          The initial manual provisioned throughput is equal to the current autoscale maximum RU/s. 
          
          Example: Suppose you have an autoscale database or container with maximum RU/s of 20,000 RU/s (scales between 2,000 - 20,000 RU/s). When you update to use manual provisioned throughput, the initial throughput is 20,000 RU/s. 
          
      - question: |
          Can I use the Azure CLI, PowerShell, or Azure Resource Manager to manage databases or containers that use autoscale?
        answer: |
          Yes. To enable autoscale on an existing database or container programatically, you can use [Azure CLI](how-to-provision-autoscale-throughput.md#azure-cli) or [PowerShell](how-to-provision-autoscale-throughput.md#azure-powershell). To create a new database or container that uses autoscale, you can use the [Azure CLI](how-to-provision-autoscale-throughput.md#azure-cli), [PowerShell](scripts/powershell/sql/throughput.md?toc=%2fpowershell%2fmodule%2ftoc.json), or [Azure Resource Manager template](./nosql/samples-resource-manager-templates.md).

      - question: |
          Is autoscale supported for shared throughput databases?
        answer: |
          Yes. To enable, autoscale for shared throughput databases, select autoscale and the **Provision throughput** option when you create the database. 

      - question: |
          How many allowed containers are allowed per shared throughput database when autoscale is enabled?
        answer: |
          Azure Cosmos DB enforces a maximum of 25 containers in a shared throughput database. The maximum applies to databases that have either autoscale or standard (manual) throughput. 

      - question: |
          How does autoscale affect the database consistency level?
        answer: |
          Autoscale has no effect on the consistency level of a database.

          For more information, see [Consistency levels](consistency-levels.md).
          
      - question: |
          What storage limit is associated with each maximum RU/s option?  
        answer: |
          The storage limit in GB for each maximum RU/s is the maximum RU/s of the database or container divided by 100. For example, if the maximum RU/s is 20,000 RU/s, the resource can support 200 GB of storage.

          For the available maximum RU/s and storage options, see [Provision throughput autoscale limits](provision-throughput-autoscale.md#autoscale-limits). 
          
      - question: |
          What happens if I exceed the storage limit that's associated with my maximum throughput?
        answer: |
          If the storage limit that's associated with the maximum throughput of the database or container is exceeded, Azure Cosmos DB automatically increases the maximum throughput to the next highest RU/s that can support that level of storage.
          
          For example, if you start with a maximum RU/s of 50,000 RU/s (scales between 5,000 and 50,000 RU/s), you can store up to 500 GB of data. If you exceed 500 GB, storage is now 600 GB and the new maximum RU/s is 60,000 RU/s (scales between 6,000 and 60,000 RU/s).
          
      - question: |
          Can I change the maximum RU/s on a database or container? 
        answer: |
          Yes. For more information, see [How to provision autoscale throughput](how-to-provision-autoscale-throughput.md).
          
          When you change the maximum RU/s, depending on the requested value, the asynchronous operation might take 4 to 6 hours to finish. [Learn more](scaling-provisioned-throughput-best-practices.md#background-on-scaling-rus).
          
      - question: |
          How do I increase the maximum RU/s?
        answer: |
          When you send a request to increase the maximum RU/s `Tmax`, depending on the maximum RU/s selected, the service provisions more resources to support the higher maximum RU/s. While this is happening, your existing workload and operations aren't affected. The system continues to scale your database or container between the previous `0.1*Tmax` to `Tmax` until the new scale range of `0.1*Tmax_new` to `Tmax_new` is ready.

      - question: |
          How do I lower the maximum RU/s?
        answer: |
          When you lower the maximum RU/s, the minimum value you can set it to is `MAX(1,000, highest maximum RU/s ever provisioned / 10, current storage in GB * 100)`, rounded to the nearest 1,000 RU/s. 
          
          Example #1: Suppose you have an autoscale container with maximum RU/s of 20,000 RU/s (scales between 2,000 and 20,000 RU/s) and 50 GB of storage. The lowest, minimum value you can set maximum RU/s to is `MAX(1,000, 20,000 / 10, **50 * 100**) = 5,000 RU/s` (scales between 500 and 5,000 RU/s).
          
          Example #2: Suppose you have an autoscale container with maximum RU/s of 100,000 RU/s and 100 GB of storage. Now, you scale maximum RU/s up to 150,000 RU/s (scales between 15,000 and 150,000 RU/s). The lowest, minimum value you can now set maximum RU/s to is `MAX(1,000, **150,000 / 10**, 100 * 100) = 15,000 RU/s` (scales between 1,500 and 15,000 RU/s). 
          
          For a shared throughput database, when you lower the maximum RU/s, the minimum value you can set it to is `MAX(1,000, highest maximum RU/s ever provisioned / 10, current storage in GB * 100,  1,000 + (MAX(Container count - 25, 0) * 1,000))`, rounded to the nearest 1,000 RU/s.  
          
          These formulas and examples relate to the minimum autoscale maximum RU/s you can set. They are distinct from the `0.1 * Tmax` to `Tmax` range the system automatically scales between. No matter what the maximum RU/s is, the system always scales between `0.1 * Tmax` to `Tmax`. 
          
      - question: |
          How does TTL work with autoscale?
        answer: |
          With autoscale, TTL operations don't affect the scaling of RU/s. Any RUs that are consumed due to TTL aren't part of the billed RU/s of the autoscale container. 
          
          For example, for an autoscale container that has 400 to 4,000 RU/s:
          - Hour 1: T=0: The container has no usage (no TTL or workload requests). The billable RU/s is 400 RU/s.
          - Hour 1: T=1: TTL is enabled.
          - Hour 1: T=2: The container starts getting requests, which consume 1,000 RU in 1 second. Also, 200 RUs worth of TTL are included. 
          The billable RU/s is still 1,000 RU/s. Regardless of when the TTL deletes occur, they don't affect the autoscale scaling logic.
          
      - question: |
          How does maximum RU/s map to physical partitions?
        answer: |
          When you first select the maximum RU/s, Azure Cosmos DB provisions by dividing the maximum RU/s by 10,000 RU/s to get the number of physical partitions. Each [physical partition](partitioning-overview.md#physical-partitions) can support up to 10,000 RU/s and 50 GB of storage. As storage size grows, Azure Cosmos DB automatically splits the partitions to add more physical partitions to handle the storage increase. If storage [exceeds the associated limit](#what-storage-limit-is-associated-with-each-maximum-ru-s-option---), Azure Cosmos DB increases the maximum RU/s. 
          
          The maximum RU/s of the database or container is divided evenly across all physical partitions. So, the total throughput any single physical partition can scale to is the maximum RU/s of the database or container divided by the number of physical partitions. 
          
      - question: |
          What happens if incoming requests exceed the maximum RU/s of the database or container?
        answer: |
          If the overall consumed RU/s exceeds the maximum RU/s of the database or container, requests that exceed the maximum RU/s are throttled and return a 429 status code. Requests that result in more than 100% normalized utilization are throttled. Normalized utilization is defined as the maximum of the RU/s utilization across all physical partitions. For example, suppose your maximum throughput is 20,000 RU/s and you have two physical partitions, P_1 and P_2, each capable of scaling to 10,000 RU/s. In any given second, if P_1 has used 6,000 RUs, and P_2 has used 8,000 RUs, the normalized utilization is `MAX(6,000 RU / 10,000 RU, 8,000 RU / 10,000 RU) = 0.8`.
          
          > [!NOTE]
          > The Azure Cosmos DB client SDKs and data import tools (Azure Data Factory, the bulk executor library) automatically retry after a 429 error is returned, so occasional 429 errors aren't problematic. A sustained high number of 429 errors might indicate that you need to increase the maximum RU/s or review your partitioning strategy for a [hot partition](#can-throttling-or-rate-limiting-errors-occur-when-autoscale-is-enabled).
          
      - question: Can throttling or rate limiting errors occur when autoscale is enabled? 
        answer: |
          Yes. It's possible to see 429 errors in two scenarios. First, when the overall consumed RU/s exceeds the maximum RU/s of the database or container, the service throttles requests accordingly. 
          
          Second, if a logical partition key value has a disproportionately higher amount of requests compared to other partition key values, like a hot partition, it's possible for the underlying physical partition to exceed its RU/s budget. As a best practice, to avoid hot partitions [choose a good partition key](partitioning-overview.md#choose-partitionkey) that results in an even distribution of both storage and throughput. 
          
          For example, if you select the 20,000 RU/s maximum throughput option and have 200 GB of storage, with four physical partitions, each physical partition can be autoscaled up to 5,000 RU/s. If there was a hot partition on a specific logical partition key, you'll see 429 errors when the underlying physical partition it resides in exceeds 5,000 RU/s. That is, when it exceeds 100% normalized utilization.

          Seeing 429 errors when you use autoscale  doesn't necessarily mean that there's an issue with your database or container. Generally for a production workload, if between 1% and 5% of requests have 429 errors and your end-to-end latency is acceptable, the errors are a healthy sign that the RU/s are being fully utilized. No action is required. Learn more about how to [interpret and debug 429 rate limiting errors](sql/troubleshoot-request-rate-too-large.md).

      - question: Can normalized RU/s consumption be 100% even if autoscale doesn't scale to the maximum RU/s?
        answer: |
          Yes. For more information, see [Monitor normalized RU/s](monitor-normalized-request-units.md#normalized-ru-consumption-and-autoscale).

          
additionalContent: |

  ## Next steps
  
  * Learn how to [enable autoscale on an Azure Cosmos DB database or container](how-to-provision-autoscale-throughput.md).
  * Learn about the [benefits of provisioned throughput with autoscale](provision-throughput-autoscale.md#benefits-of-autoscale).
  * Learn more about [logical and physical partitions](partitioning-overview.md).
