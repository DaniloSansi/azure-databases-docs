---
title: How to use Azure Cosmos DB data with Azure OpenAI
description: Get started developing a Python application that works with Azure Cosmos DB for NoSQL. This article helps you learn how to set up a project and configure access to an Azure Cosmos DB for NoSQL endpoint.
author: jacodel
ms.author: sidandrews
ms.service: cosmos-db
ms.subservice: nosql
ms.topic: how-to
ms.date: 07/25/2023
ms.custom: OpenAI, vector-search, RAG
---

# Use Cosmos DB with Azure OpenAI 

Harnessing the power of Azure OpenAI models, developers can now leverage the rich and diverse data stored in Azure Cosmos DB to create highly customized AI applications. The process of using data with large language models is called Retrieval Augmented Generation (RAG). 

This document starts off with an overview of some core terms and concepts used in RAG, LLMs, and AI. Then it transitions into real-world applications and implementation details providing links to documentation, end-to-end tutorials, assets, and code snippets, all of which will provide you the tools you need to build AI powered applications using Azure Cosmos DB and Azure OpenAI. 


## Concepts

### AI Models
Artificial Intelligence (AI) models are software programs that have undergone training on specific datasets to carry out particular tasks, including pattern recognition. These models employ algorithms to learn from the provided training data and apply their acquired knowledge to achieve predefined objectives. They find applications in various fields such as computer vision, robotics, and natural language processing. Azure OpenAI offers a range of models designed for natural language processing, each with varying levels of power and speed to cater to different scenarios. Specifically, we'll be working with models to create vector embeddings and Large Language Models (LLMs) like the GPT-3.5 and GPT-4 series. You can find the list of available models at [Azure OpenAI Service models - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models)

### Prompts
A prompt refers to a specific text or information that functions as an instruction or directive to an LLM. It serves as sort of guide or directive, indicating the desired response or output and providing guidance on how to formulate it. A prompt can take various forms, such as a question, a statement, or even a code snippet. To create effective prompts for AI systems, it's important to grasp the fundamentals of chatbot systems, comprehend the distinctions between popular models, construct clear and concise messages, and continuously refine and test the prompts to ensure optimal performance.

#### Prompt engineering

The objective of prompt engineering is to create and refine prompts that strike a balance between specificity and clarity, ensuring they effectively elicit the desired output from the model. These prompts should be tailored to capture the complete spectrum of relevant information while considering the capabilities and limitations of LLMs. By carefully crafting prompts, we equip chatbots with the necessary guidance to generate the most suitable and contextually appropriate responses.

To learn more check out [Azure OpenAI Service - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/prompt-engineering) and [Prompt engineering techniques with Azure OpenAI - Azure OpenAI Service | Microsoft Learn](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions)

### Tokens

Tokens are units of text that are generated by splitting the input text into smaller segments. These segments can either be words or groups of characters, varying in length from a single character to an entire word. For instance, the word "hamburger" would be divided into tokens such as "ham," "bur," and "ger," while a short and common word like "pear" would be considered a single token. It's common for many tokens to begin with a whitespace, such as "hello" and "bye."

Prior to processing prompts using the Azure OpenAI API, the input text is segmented into tokens. These tokens are not precisely divided at word boundaries but can include trailing spaces and even sub-words. The number of tokens processed in each API request depends on factors such as the length of the input, output, and request parameters. The quantity of tokens being processed also impacts the response time and throughput of the models. However, it's important to note that there exists a limit to the amount of text that can be sent due to the token limit imposed by each model, restricting the number of tokens that can be consumed in a single request/response from Azure OpenAI.

### Vectors, embeddings, and vector search

*Vectors* are ordered arrays of numbers (typically floats) that represents information about some data. The process for turning data into a vector is called *vectorization*. 

*Embeddings* are vectors that represent important features of data. Embeddings are often learned by using a deep learning model, and can be utilized by machine learning and AI models. Azure OpenAI features models for creating embeddings from text data. The service breaks text out into tokens and generates embeddings using models pre-trained by OpenAI.

 [Learn more about creating embeddings with Azure OpenAI here.](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/understand-embeddings) 

*Vector search* refers to the the process of finding all vectors in a dataset that are similar to a specific query vector. Similarity is measured using a distance metric (like cosine similarity). For example, if your data consists of thousands of documents, and you have a query document and want to find the most similar document in your data, you can create embeddings from your documents using Azure OpenAI, then perform a vector search to find the most similar documents from your dataset.

<!-- 
### Vector search

:::image type="content" source="media/how-to-multi-master/enable-multi-region-writes.png" alt-text="A green background with black text that says Align."::: -->


### Retrieval Augmented Generation (RAG)
Retrieval-Augmented Generation, or RAG, involves the process of retrieving supplementary data to provide the LLM with the ability to use this data when it generates repspones. For example, when presented with a user's question or prompt, RAG aims to select the most pertinent and current domain-specific knowledge from external sources, such as articles or documents. This retrieved information serves as a valuable reference for the model when generating its response.

The RAG pattern, in conjunction with prompt engineering, serves the purpose of enhancing response quality by offering additional contextual information to the model. By incorporating relevant external sources into the generation process, RAG enables the model to leverage a broader knowledge base, resulting in more comprehensive and informed responses. This also known as “grounding” LLMs, which is explained more here: Grounding LLMs - Microsoft Community Hub 

## Getting started with Azure Cosmos DB and Azure OpenAI

To learn how to create an Azure OpenAI resource and deploy models, see the [How-to Guide](../../ai-services/openai/how-to/create-resource?pivots=web-portal)


### Using Cosmos DB data with Azure OpenAI
The RAG pattern harnesses external knowledge and models to effectively handle custom data or domain-specific knowledge. It involves extracting pertinent information from an external data source and integrating it into the model request through prompt engineering.

Considering the limitation of a restricted number of tokens per request, a robust mechanism is necessary to identify the most relevant data from the external source that can be passed to the model. This is where embeddings play a crucial role. By converting the data in our database into embeddings and storing them as vectors for future use, we leverage the advantage of capturing the semantic meaning of the text, going beyond mere keywords to comprehend the context.

Prior to sending a request to Azure OpenAI, the user input/query/request is also transformed into an embedding, and vector search techniques are employed to locate the most similar embeddings within the database. This enables the identification of the most relevant data records in the database. These retrieved records are then supplied as input to the model request using prompt engineering.

## PLACEHOLDER FOR LINKS to tutorials and code

There are several ways to utilize Cosmos DB data with Azure OpenAI, including:
•	Azure Cosmos DB Mongo DB API vCore: Utilizing Vector Search capabilities.
•	Azure Cosmos DB NoSQL API: Utilizing Azure Cognitive Search. 
•	Azure Cosmos DB NoSQL API: Leveraging Redis Vector Search.
•	Azure Cosmos DB for PostgreSQL: use pgvector
