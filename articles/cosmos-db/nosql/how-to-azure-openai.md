---
title: How to use Azure Cosmos DB data with Azure OpenAI
description: Get started developing a Python application that works with Azure Cosmos DB for NoSQL. This article helps you learn how to set up a project and configure access to an Azure Cosmos DB for NoSQL endpoint.
author: jacodel
ms.author: sidandrews
ms.service: cosmos-db
ms.subservice: nosql
ms.topic: how-to
ms.date: 07/25/2023
ms.custom: OpenAI, vector-search, RAG
---

# Use Cosmos DB data with Azure OpenAI 

The Large Language Models (LLMs) in Azure OpenAI are incrediblly powerful tools that can take your AI-powered applications to the next level. The utility of LLMs can increase significantly when the models can have access to the right data, at the right time, from your application's data store. This process is known as Retrieval Augmented Generation (RAG) and there are many ways to do this today with Azure Cosmos DB.

In this document, we'll review key concepts for RAG and then provide links to tutorials and sample code that demonstrate some of most powerful RAG patterns using *vector search* to bring the most semantically relevant data to your LLMs. These will help you quickly become comfortable with leveraging your Azure Cosmos DB data with Azure OpenAI models to empower your AI applications with your own data.

To jump right into the tutorial and sample code, use the links below:

1. **[RAG with Azure Cosmos DB NoSQL with Azure Cognitive Search](#azure-cosmos-db-nosql-and-azure-cognitive-search)**.  Augment your Cosmos DB data with the powerful semantic and vector search capabilities of Azure Cognitive Search.
2. **[RAG with Azure Cosmos DB for Mongo DB vCore](#azure-cosmos-db-for-mongodb-vcore)**. Featuring native support for vector search, store your application data and vector embeddings together in a single MongoDB compatible service. 
3. **[RAG with Azure Cosmos DB for PostgreSQL](#azure-cosmos-db-for-postgresql)**. Offering native support vector search, you can store your data and vectors together in a scalable PostgreSQL offering.

## Key concepts

### Retrieval Augmented Generation (RAG)
Retrieval-Augmented Generation, or RAG, involves the process of retrieving supplementary data to provide the LLM with the ability to use this data when it generates repspones. For example, when presented with a user's question or prompt, RAG aims to select the most pertinent and current domain-specific knowledge from external sources, such as articles or documents. This retrieved information serves as a valuable reference for the model when generating its response.


The RAG pattern, in conjunction with prompt engineering, serves the purpose of enhancing response quality by offering additional contextual information to the model. By incorporating relevant external sources into the generation process, RAG enables the model to leverage a broader knowledge base, resulting in more comprehensive and informed responses. This also known as “grounding” LLMs, which is explained more here: Grounding LLMs - Microsoft Community Hub 

### Prompts and prompt engineering
A prompt refers to a specific text or information that can serve as an instruction to an LLM, or as contextual data that the LLM can build upon. A prompt can take various forms, such as a question, a statement, or even a code snippet. Prompts can serve as  *instructions* that provide directives to the LLM, *primary content* that gives information to the LLM for processing, *examples* that help condition the model to a particular task or process, *cues* that can help point direct the LLM's output in the right direction, or *supporting content*, which can be supplemental information the LLM can use to generate output. The process of creating good prompts for a scenario is called *prompt engineering*. To learn more about prompts and best practices for prompt engineering, check out [Azure OpenAI Service - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/prompt-engineering) 


### Tokens
Tokens are small chunks text that are generated by splitting the input text into smaller segments. These segments can either be words or groups of characters, varying in length from a single character to an entire word. For instance, the word "hamburger" would be divided into tokens such as "ham," "bur," and "ger," while a short and common word like "pear" would be considered a single token. 

In Azure OpenAI, input text provided to the API is turned into tokens (tokenized). The number of tokens processed in each API request depends on factors such as the length of the input, output, and request parameters. The quantity of tokens being processed also impacts the response time and throughput of the models. There are limits to the amount tokens each model can take in a single request/response from Azure OpenAI. [Learn more about AzureOpenAI Service quotas and limits here](https://learn.microsoft.com/azure/ai-services/openai/quotas-limits)

### Vectors, embeddings, and vector search

#### Vectors
Vectors are ordered arrays of numbers (typically floats) that can represent information about some data. For example, an image can be represented as a vector of pixel values, or a string of text can be represented as a vector or ASCII values. The process for turning data into a vector is called *vectorization*. 

#### Embeddings 
Embeddings are vectors that represent important features of data. Embeddings are often learned by using a deep learning model, and can be utilized by other machine learning and AI models as features. Embeddings can also capture semantic similarity between similar concepts. For example, in generating an embedding for the words "person" and "human", we would expect their embeddings (vector representation) to be similar in value since the words are also semantically similar.

 Azure OpenAI features models for creating embeddings from text data. The service breaks text out into tokens and generates embeddings using models pre-trained by OpenAI.[Learn more about creating embeddings with Azure OpenAI here.](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/understand-embeddings) 

#### Vector search
Vector search refers to the the process of finding all vectors in a dataset that are semantically similar to a specific query vector. Keeping with the above example, if I have a query vector for the word "human", and I search the entire dictionary for semantically similar words, I would expect to find the word "person" as a close match. This closeness, or distance, is measured using a similarity metric such as cosine similarity. The more similar the vectors are, the smaller the distance between them.

If your data consists of thousands or millions of documents, and you have a query document and want to find the most similar document in your data, you can create embeddings for your data and the query document using Azure OpenAI, then perform a vector search to find the most similar documents from your dataset. However, erforming a vector search across a few examples is trivial, however doing this across thousands or millions of data points becomes challenging. There are also trade-offs between exhaustive search and approximate nearest neighbor (ANN) search methods including latency, throughput, accuracy, and cost, all of which can depend on the requirements of your application.

Adding Azure Cosmos DB vector search capabilities to Azure OpenAI Service enables you to store long term memory and chat history to improve your Large Language Model (LLM) solution. Vector search allows you to efficiently query back the most relevant context to personalize Azure OpenAI prompts in a token-efficient manner. Storing vector embeddings alongside the data in an integrated solution minimizes the need to manage data synchronization and accelerates your time-to-market for AI app development. 

 With Cosmos DB, there are several products that can perform vector search at scale. The infographic outlines three options:

 :::image type="content" source="includes/media/use-data-with-openai/cosmosdb-vector-search.png" alt-text="Use Vector Search with Azure Cosmos DB to inform your AI models":::


## Get started using Azure Cosmos DB with Azure OpenAI

### Azure Cosmos DB NoSQL and Azure Cognitive Search

Implement RAG (Retrieval-Augmented Generation) with Azure Cosmos DB NoSQL and Azure Cognitive Search. This approach enables powerful integration of your data residing in Azure Cosmos DB NoSQL into your AI-oriented applications. Azure Cognitive Search empowers you to efficiently store, index, and query high-dimensional vector data, which is directly stored in Azure Cosmos DB NoSQL. 

 :::image type="content" source="includes/media/use-data-with-openai/RAG-cdb-cs.png" alt-text="RAG architeture with Azure Cosmos DB NoSQL and Azure Cognitive Search":::

#### Code samples
- [.NET retail chatbot demo](https://github.com/AzureCosmosDB/VectorSearchAiAssistant/tree/cognitive-search-vector-v2)
- [.NET tutorial - recipe chatbot](https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples/tree/combining-assets/CosmosRecipeGuide_NoSQLwithMongoDBVectorSearch)
- [.Net samples - Hackathon project](https://github.com/AzureCosmosDB/OpenAIHackathon)
- [Python notebook tutorial - Azure product chatbot](https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples/tree/main/Notebooks)

### Azure Cosmos DB for MongoDB vCore
Carry out RAG (Retrieval-Augmented Generation) by leveraging vector search in Azure Cosmos DB for MongoDB vCore, facilitating a smooth merger of your AI-centric applications with your stored data in Azure Cosmos DB. The use of vector search offers an efficient way to store, index, and search high-dimensional vector data directly within Azure Cosmos DB for MongoDB vCore. This approach eradicates the necessity of migrating your data to pricier alternatives for availing vector search functionalities.

 :::image type="content" source="includes/media/use-data-with-openai/RAG-cdb-vcore.png" alt-text="RAG architeture with Azure Cosmos DB for MongoDB vCore":::

#### Code samples
- [.NET retail chatbot demo](https://github.com/AzureCosmosDB/VectorSearchAiAssistant/tree/mongovcorev2)
- [.NET tutorial - recipe chatbot](https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples/tree/main/RecipeGuide_CosmosDBMongoVCoreOpenAI)

### Azure Cosmos DB for PostgreSQL
You can now employ RAG (Retrieval-Augmented Generation) by utilizing vector search within Azure Cosmos DB for PostgreSQL. This strategy provides a seamless integration of your AI-driven applications, including the ones developed using Azure OpenAI embeddings, with your data housed in Azure Cosmos DB. By taking advantage of vector search, you can effectively store, index, and execute queries on high-dimensional vector data directly within Azure Cosmos DB for PostgreSQL. This approach negates the necessity of relocating your data to costlier platforms for harnessing vector search functionalities.

 :::image type="content" source="includes/media/use-data-with-openai/RAG-cdb-pg.png" alt-text="RAG architeture with Azure Cosmos DB for PostgreSQL":::

#### Code samples
- Python: [Python notebook tutorial - food review chatbot ](https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples/tree/main/CosmosDB-PostGres-CogSearchVector)


### Using Cosmos DB data with Azure OpenAI
The RAG pattern harnesses external knowledge and models to effectively handle custom data or domain-specific knowledge. It involves extracting pertinent information from an external data source and integrating it into the model request through prompt engineering.

Considering the limitation of a restricted number of tokens per request, a robust mechanism is necessary to identify the most relevant data from the external source that can be passed to the model. This is where embeddings play a crucial role. By converting the data in our database into embeddings and storing them as vectors for future use, we leverage the advantage of capturing the semantic meaning of the text, going beyond mere keywords to comprehend the context.

Prior to sending a request to Azure OpenAI, the user input/query/request is also transformed into an embedding, and vector search techniques are employed to locate the most similar embeddings within the database. This enables the identification of the most relevant data records in the database. These retrieved records are then supplied as input to the model request using prompt engineering.

#### Documentation for vector search with Azure Cosmos DB
 - [Azure Cognitive Search](https://learn.microsoft.com/azure/search/vector-search-overview)
 - [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/azure/cosmos-db/mongodb/vcore/vector-search)
 - [Azure Cosmos DB PostgreSQL](https://learn.microsoft.com/azure/cosmos-db/postgresql/howto-use-pgvector)
